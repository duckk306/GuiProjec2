{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665322bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from analysis.analyzer import TTTH_Analyzer\n",
    "from processor.feature import FeatureProcessor\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "# from underthesea import word_tokenize, pos_tag, sent_tokenize\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from pyvi import ViTokenizer\n",
    "from underthesea import word_tokenize\n",
    "from gensim import corpora, models, similarities\n",
    "from tqdm import tqdm\n",
    "_analyzer = TTTH_Analyzer()\n",
    "_processor = FeatureProcessor()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a030a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_motobikes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bd1e801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Ti√™u ƒë·ªÅ</th>\n",
       "      <th>Gi√°</th>\n",
       "      <th>Kho·∫£ng gi√° min</th>\n",
       "      <th>Kho·∫£ng gi√° max</th>\n",
       "      <th>M√¥ t·∫£ chi ti·∫øt</th>\n",
       "      <th>Th∆∞∆°ng hi·ªáu</th>\n",
       "      <th>D√≤ng xe</th>\n",
       "      <th>NƒÉm ƒëƒÉng k√Ω</th>\n",
       "      <th>S·ªë Km ƒë√£ ƒëi</th>\n",
       "      <th>Lo·∫°i xe</th>\n",
       "      <th>Dung t√≠ch xe</th>\n",
       "      <th>Xu·∫•t x·ª©</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B√°n Vespa Sprint 125cc 2024 xanh d∆∞∆°ng, xe ƒë·∫πp...</td>\n",
       "      <td>66.000.000 ƒë</td>\n",
       "      <td>72.53 tr</td>\n",
       "      <td>85.14 tr</td>\n",
       "      <td>B√°n xe #Vespa Sprint 125cc. Mua m·ªõi t·∫°i #Topco...</td>\n",
       "      <td>Piaggio</td>\n",
       "      <td>Vespa</td>\n",
       "      <td>2024</td>\n",
       "      <td>14000</td>\n",
       "      <td>Tay ga</td>\n",
       "      <td>100 - 175 cc</td>\n",
       "      <td>ƒêang c·∫≠p nh·∫≠t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>üî•üî•SH 150i Th·∫Øng ABS 2019 BSTP Ch√≠nh Ch·ªß</td>\n",
       "      <td>79.500.000 ƒë</td>\n",
       "      <td>62.76 tr</td>\n",
       "      <td>73.68 tr</td>\n",
       "      <td>_B√°n SH 150i Th·∫Øng ABS 2019 X√°m B·∫°c, √öp Team X...</td>\n",
       "      <td>Honda</td>\n",
       "      <td>SH</td>\n",
       "      <td>2019</td>\n",
       "      <td>28000</td>\n",
       "      <td>Tay ga</td>\n",
       "      <td>100 - 175 cc</td>\n",
       "      <td>ƒêang c·∫≠p nh·∫≠t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CC Vision Th·ªÉ Thao 2023 ƒêen+b·ªô ƒë√®n Demi audi A7</td>\n",
       "      <td>37.000.000 ƒë</td>\n",
       "      <td>28 tr</td>\n",
       "      <td>32.86 tr</td>\n",
       "      <td>Ch√≠nh ch·ªß b√°n Vision phi√™n b·∫£n Th·ªÉ Thao 2023 ƒê...</td>\n",
       "      <td>Honda</td>\n",
       "      <td>Vision</td>\n",
       "      <td>2023</td>\n",
       "      <td>12000</td>\n",
       "      <td>Tay ga</td>\n",
       "      <td>100 - 175 cc</td>\n",
       "      <td>ƒêang c·∫≠p nh·∫≠t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            Ti√™u ƒë·ªÅ           Gi√°  \\\n",
       "0   1  B√°n Vespa Sprint 125cc 2024 xanh d∆∞∆°ng, xe ƒë·∫πp...  66.000.000 ƒë   \n",
       "1   2            üî•üî•SH 150i Th·∫Øng ABS 2019 BSTP Ch√≠nh Ch·ªß  79.500.000 ƒë   \n",
       "2   3    CC Vision Th·ªÉ Thao 2023 ƒêen+b·ªô ƒë√®n Demi audi A7  37.000.000 ƒë   \n",
       "\n",
       "  Kho·∫£ng gi√° min Kho·∫£ng gi√° max  \\\n",
       "0       72.53 tr       85.14 tr   \n",
       "1       62.76 tr       73.68 tr   \n",
       "2          28 tr       32.86 tr   \n",
       "\n",
       "                                      M√¥ t·∫£ chi ti·∫øt Th∆∞∆°ng hi·ªáu D√≤ng xe  \\\n",
       "0  B√°n xe #Vespa Sprint 125cc. Mua m·ªõi t·∫°i #Topco...     Piaggio   Vespa   \n",
       "1  _B√°n SH 150i Th·∫Øng ABS 2019 X√°m B·∫°c, √öp Team X...       Honda      SH   \n",
       "2  Ch√≠nh ch·ªß b√°n Vision phi√™n b·∫£n Th·ªÉ Thao 2023 ƒê...       Honda  Vision   \n",
       "\n",
       "  NƒÉm ƒëƒÉng k√Ω  S·ªë Km ƒë√£ ƒëi Lo·∫°i xe  Dung t√≠ch xe        Xu·∫•t x·ª©  \n",
       "0        2024        14000  Tay ga  100 - 175 cc  ƒêang c·∫≠p nh·∫≠t  \n",
       "1        2019        28000  Tay ga  100 - 175 cc  ƒêang c·∫≠p nh·∫≠t  \n",
       "2        2023        12000  Tay ga  100 - 175 cc  ƒêang c·∫≠p nh·∫≠t  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.drop(columns=['ƒê·ªãa ch·ªâ','T√¨nh tr·∫°ng',\n",
    "       'Ch√≠nh s√°ch b·∫£o h√†nh', 'Tr·ªçng l∆∞·ª£ng', 'Href'])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f69dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_numeric before fill missing values: 65\n",
      "cc_numeric after fill missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# S·∫Øp x·∫øp d·ªØ li·ªáu theo Th∆∞∆°ng hi·ªáu, D√≤ng xe, Lo·∫°i xe (tƒÉng d·∫ßn)\n",
    "data = data.sort_values(by=['Th∆∞∆°ng hi·ªáu', 'D√≤ng xe', 'Lo·∫°i xe'], ascending=[True, True, True])\n",
    "# Reset l·∫°i index sau khi s·∫Øp x·∫øp\n",
    "data = data.reset_index(drop=True)\n",
    "###############################\n",
    "\n",
    "#Chu·∫©n h√≥a c·ªôt \"Gi√°\"\n",
    "data['Gi√°'] = (\n",
    "    data['Gi√°']\n",
    "    .astype(str)\n",
    "    .str.replace(r'[^\\d]', '', regex=True)  # lo·∫°i b·ªè m·ªçi k√Ω t·ª± kh√¥ng ph·∫£i s·ªë\n",
    ")\n",
    "# ƒê·ªïi chu·ªói r·ªóng th√†nh NaN\n",
    "data.loc[data['Gi√°'] == '', 'Gi√°'] = np.nan\n",
    "# √âp ki·ªÉu float v√† chia cho 1,000,000 ƒë·ªÉ ra ƒë∆°n v·ªã tri·ªáu\n",
    "data['Gi√°'] = data['Gi√°'].astype(float) / 1_000_000 \n",
    "\n",
    "for col in ['Kho·∫£ng gi√° min', 'Kho·∫£ng gi√° max']:\n",
    "    data[col] = (\n",
    "        data[col]\n",
    "        .astype(str)\n",
    "        .str.replace('tr', '', case=False, regex=False)  # b·ªè ch·ªØ \"tr\"\n",
    "        .str.replace(',', '.')  # n·∫øu c√≥ d·∫•u ph·∫©y\n",
    "        .str.strip()  # b·ªè kho·∫£ng tr·∫Øng\n",
    "    )\n",
    "\n",
    "    # ƒê·ªïi chu·ªói r·ªóng th√†nh NaN r·ªìi √©p ki·ªÉu float\n",
    "    data.loc[data[col] == '', col] = np.nan\n",
    "    data[col] = data[col].astype(float)\n",
    "###############################\n",
    "data_clean = data.copy()\n",
    "# 1. X√≥a d√≤ng thi·∫øu ti√™u ƒë·ªÅ ho·∫∑c gi√°\n",
    "data_clean = data_clean.dropna(subset=['Ti√™u ƒë·ªÅ', 'Gi√°'])\n",
    "\n",
    "# 2. ƒêi·ªÅn kho·∫£ng gi√° min/max b·∫±ng c·ªôt Gi√°\n",
    "data_clean['Kho·∫£ng gi√° min'] = data_clean['Kho·∫£ng gi√° min'].fillna(data_clean['Gi√°'])\n",
    "data_clean['Kho·∫£ng gi√° max'] = data_clean['Kho·∫£ng gi√° max'].fillna(data_clean['Gi√°'])\n",
    "\n",
    "# 3. N·∫øu v·∫´n c√≤n NaN, ƒëi·ªÅn median theo Th∆∞∆°ng hi·ªáu\n",
    "data_clean['Kho·∫£ng gi√° min'] = data_clean.groupby('Th∆∞∆°ng hi·ªáu')['Kho·∫£ng gi√° min'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "data_clean['Kho·∫£ng gi√° max'] = data_clean.groupby('Th∆∞∆°ng hi·ªáu')['Kho·∫£ng gi√° max'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "#############################################################\n",
    "\n",
    "def price_segment(price):\n",
    "    \"\"\"\n",
    "    Ph√¢n lo·∫°i xe theo ph√¢n kh√∫c gi√°.\n",
    "    - Ph·ªï th√¥ng: < 70 tri·ªáu\n",
    "    - C·∫≠n cao c·∫•p: 70‚Äì200 tri·ªáu\n",
    "    - Cao c·∫•p: > 200 tri·ªáu\n",
    "    \"\"\"\n",
    "    if price < 70:\n",
    "        return \"Ph·ªï th√¥ng\"\n",
    "    elif price < 200:\n",
    "        return \"C·∫≠n cao c·∫•p\"\n",
    "    else:\n",
    "        return \"Cao c·∫•p\"\n",
    "\n",
    "data_clean[\"Ph√¢n kh√∫c gi√°\"] = data_clean[\"Gi√°\"].apply(price_segment)\n",
    "##############################################################\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ numeric\n",
    "data_clean[['Gi√°', 'Kho·∫£ng gi√° min', 'Kho·∫£ng gi√° max']] = data_clean[\n",
    "    ['Gi√°', 'Kho·∫£ng gi√° min', 'Kho·∫£ng gi√° max']\n",
    "].astype(float)\n",
    "# L·ªçc b·ªè c√°c gi√° b·∫•t th∆∞·ªùng\n",
    "data_clean = data_clean[(data_clean['Gi√°'] > 1) & (data_clean['Gi√°'] < 5000)]\n",
    "################################################################\n",
    "\n",
    "# X·ª≠ l√Ω c·ªôt S·ªë Km ƒë√£ ƒëi\n",
    "data_clean.loc[data_clean['S·ªë Km ƒë√£ ƒëi'] > 99999, 'S·ªë Km ƒë√£ ƒëi'] = 99999\n",
    "################################################################\n",
    "\n",
    "\n",
    "# L√†m s·∫°ch text\n",
    "for col in ['Th∆∞∆°ng hi·ªáu', 'D√≤ng xe', 'Lo·∫°i xe', 'Dung t√≠ch xe', 'Xu·∫•t x·ª©', 'Ph√¢n kh√∫c gi√°']:\n",
    "    data_clean[col] = data_clean[col].str.strip().str.title()\n",
    "\n",
    "# Dung t√≠ch xe: map ƒë·ªãnh l∆∞·ª£ng\n",
    "def parse_cc(val):\n",
    "    if 'D∆∞·ªõi' in val: return 40\n",
    "    if '50 - 100' in val: return 75\n",
    "    if '100 - 175' in val: return 137\n",
    "    if 'Tr√™n 175' in val: return 200\n",
    "    return np.nan\n",
    "data_clean['cc_numeric'] = data_clean['Dung t√≠ch xe'].apply(parse_cc)\n",
    "######################################################################\n",
    "\n",
    "# Ph√¢n kh√∫c gi√°: map ordinal\n",
    "price_segment_map = {'Ph·ªï Th√¥ng': 1, 'C·∫≠n Cao C·∫•p': 2, 'Cao C·∫•p': 3}\n",
    "data_clean['price_segment_code'] = data_clean['Ph√¢n kh√∫c gi√°'].map(price_segment_map)\n",
    "#######################################################################\n",
    "\n",
    "# Thay c√°c gi√° tr·ªã ƒë·∫∑c bi·ªát trong c·ªôt NƒÉm ƒëƒÉng k√Ω\n",
    "data_clean['NƒÉm ƒëƒÉng k√Ω'] = data_clean['NƒÉm ƒëƒÉng k√Ω'].replace({\n",
    "    'tr∆∞·ªõc nƒÉm 1980': '1979',\n",
    "    'ƒêang c·∫≠p nh·∫≠t': np.nan,\n",
    "    'Kh√¥ng r√µ': np.nan\n",
    "})\n",
    "# Chuy·ªÉn sang ki·ªÉu int\n",
    "data_clean['NƒÉm ƒëƒÉng k√Ω'] = pd.to_numeric(data_clean['NƒÉm ƒëƒÉng k√Ω'], errors='coerce')\n",
    "data_clean['NƒÉm ƒëƒÉng k√Ω'] = data_clean['NƒÉm ƒëƒÉng k√Ω'].astype(int)\n",
    "\n",
    "min_age = 0.5  # t√≠nh tr√≤n cho 6 th√°ng\n",
    "data_clean['age'] = 2025 - data_clean['NƒÉm ƒëƒÉng k√Ω']\n",
    "\n",
    "# Thay age == 0 b·∫±ng min_age\n",
    "data_clean.loc[data_clean['age'] <= 0, 'age'] = min_age\n",
    "#######################################################################\n",
    "\n",
    "# X·ª≠ l√Ω missing values cho c·ªôt cc_numeric\n",
    "_processor.handle_missing_values_by_median('cc_numeric', data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2638481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    \"Gi√°\", \"Kho·∫£ng gi√° min\", \"Kho·∫£ng gi√° max\",\n",
    "    \"S·ªë Km ƒë√£ ƒëi\", \"age\", \"cc_numeric\"]\n",
    "\n",
    "\n",
    "# Danh s√°ch th∆∞∆°ng hi·ªáu m√¥ t√¥ cao c·∫•p\n",
    "premium_brands = ['BMW', 'Harley Davidson', 'Ducati', 'Triumph', 'Kawasaki', 'Benelli']\n",
    "\n",
    "# √Åp d·ª•ng ng∆∞·ª°ng gi√° t·ªëi ƒëa cho xe ph·ªï th√¥ng\n",
    "data_clean.loc[\n",
    "    (~data_clean['Th∆∞∆°ng hi·ªáu'].isin(premium_brands)) & (data_clean['Gi√°'] > 300),\n",
    "    'Gi√°'\n",
    "] = 300\n",
    "\n",
    "# Ph√°t hi·ªán outliers s·ª≠ d·ª•ng IQR\n",
    "Q1 = data_clean[numeric_cols].quantile(0.25)\n",
    "Q3 = data_clean[numeric_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outlier_mask = (data_clean[numeric_cols] < (Q1 - 1.5 * IQR)) | (data_clean[numeric_cols] > (Q3 + 1.5 * IQR))\n",
    "outlier_counts = outlier_mask.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c3cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªãnh nghƒ©a c√°c nh√≥m keyword\n",
    "\n",
    "# Nh√≥m 1: M·ªöI / T√åNH TR·∫†NG XE\n",
    "kw_moi = [\n",
    "    \"m·ªõi\", \"c√≤n m·ªõi\", \"nh∆∞ m·ªõi\", \"m·ªõi 95\", \"m·ªõi 99\", \"m·ªõi tinh\",\n",
    "    \"xe l∆∞·ªõt\", \"xe √≠t ƒëi\", \"√≠t s·ª≠ d·ª•ng\", \"xe ƒë·ªÉ kh√¥ng\", \"ƒë·ªÉ kho\",\n",
    "    \"keng\", \"leng keng\", \"nguy√™n zin\", \"zin 100%\", \"zin nguy√™n b·∫£n\",\n",
    "    \"d√°n keo\", \"d√°n ppf\", \"ngo·∫°i h√¨nh ƒë·∫πp\", \"d√†n √°o li·ªÅn l·∫°c\", \"ƒë·∫πp nh∆∞ h√¨nh\"\n",
    "]\n",
    "# Nh√≥m 2: ƒê·ªò XE / ƒê·ªí CH∆†I / N√ÇNG C·∫§P\n",
    "kw_do_xe = [\n",
    "    \"ƒë·ªô\", \"ƒë·ªì ch∆°i\", \"full ƒë·ªì\", \"p√¥ ƒë·ªô\", \"p√¥ m√≥c\", \"phu·ªôc rcb\", \"tay th·∫Øng\",\n",
    "    \"l√™n ƒë·ªì\", \"tem ƒë·ªô\", \"l√™n full ƒë·ªì\", \"ƒë·ªì zin c√≤n ƒë·ªß\", \"k√≠nh gi√≥\", \"th√πng givi\",\n",
    "    \"·ªëc titan\", \"m√£o gi√≥\", \"bao tay\", \"tr·ª£ l·ª±c\", \"ƒë·ªô m√°y\"\n",
    "]\n",
    "# Nh√≥m 3: M·ª®C ƒê·ªò S·ª¨ D·ª§NG\n",
    "kw_su_dung = [\n",
    "    \"√≠t ƒëi\", \"ƒëi l√†m\", \"ƒëi h·ªçc\", \"ƒëi ph∆∞·ª£t\", \"ƒëi c√† ph√™\", \"ƒë·ªÉ kh√¥ng\",\n",
    "    \"√≠t s·ª≠ d·ª•ng\", \"xe gia ƒë√¨nh\", \"xe c√¥ng ty\", \"d∆∞ xe\", \"ƒëi l·∫°i nh·∫π nh√†ng\",\n",
    "    \"xe n·ªØ d√πng\", \"xe n·ªØ ch·∫°y\", \"xe ƒë·ªÉ l√¢u\", \"√≠t ch·∫°y\", \"ƒëi g·∫ßn\"\n",
    "]\n",
    "# Nh√≥m 4: B·∫¢O D∆Ø·ª†NG / S·ª¨A CH·ªÆA\n",
    "kw_bao_duong = [\n",
    "    \"b·∫£o d∆∞·ª°ng\", \"b·∫£o tr√¨\", \"thay nh·ªõt\", \"v·ªá sinh\", \"bao test\", \"ƒëi b·∫£o d∆∞·ª°ng\",\n",
    "    \"b·∫£o d∆∞·ª°ng ƒë·ªãnh k·ª≥\", \"m·ªõi thay b√¨nh\", \"m·ªõi l√†m n·ªìi\", \"ƒë√£ l√†m l·∫°i m√°y\",\n",
    "    \"thay b·ªë th·∫Øng\", \"thay l·ªçc\", \"b·∫£o d∆∞·ª°ng l·ªõn\", \"ch·ªânh s√™n\", \"xe k·ªπ\"\n",
    "]\n",
    "# Nh√≥m 5: ƒê·ªò B·ªÄN / M√ÅY M√ìC / CH·∫§T L∆Ø·ª¢NG\n",
    "kw_do_ben = [\n",
    "    \"m√°y √™m\", \"n·ªï √™m\", \"ch·∫°y √™m\", \"m√°y m·∫°nh\", \"m√°y b·ªëc\", \"ti·∫øt ki·ªám xƒÉng\",\n",
    "    \"·ªïn ƒë·ªãnh\", \"ch·∫°y ngon\", \"kh√¥ng x√¨ nh·ªõt\", \"kh√¥ng r√≤ r·ªâ\", \"kh√¥ng l·ªói\",\n",
    "    \"m√°y kh√¥ r√°o\", \"m√°y t·ªët\", \"ch·∫°y m∆∞·ª£t\", \"v·∫≠n h√†nh ·ªïn ƒë·ªãnh\", \"√™m √°i\",\n",
    "    \"b·ªÅn b·ªâ\", \"m√°y m√≥c zin\", \"ch·∫°y b√¨nh th∆∞·ªùng\", \"ho·∫°t ƒë·ªông t·ªët\"\n",
    "]\n",
    "# Nh√≥m 6: GI·∫§Y T·ªú / PH√ÅP L√ù\n",
    "kw_phap_ly = [\n",
    "    \"ch√≠nh ch·ªß\", \"·ªßy quy·ªÅn\", \"bao sang t√™n\", \"c√† v·∫πt\", \"gi·∫•y t·ªù ƒë·∫ßy ƒë·ªß\",\n",
    "    \"gi·∫•y t·ªù h·ª£p l·ªá\", \"h·ªì s∆° g·ªëc\", \"bstp\", \"bao c√¥ng ch·ª©ng\", \n",
    "    \"bao tranh ch·∫•p\", \"ra t√™n\", \"cavet\", \"h·ª£p ph√°p\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6e3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m check t·ª´ kh√≥a xu·∫•t hi·ªán trong m√¥ t·∫£\n",
    "def keyword_flag(text: str, keywords: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Ki·ªÉm tra xem text c√≥ ch·ª©a √≠t nh·∫•t 1 t·ª´ kh√≥a trong danh s√°ch kh√¥ng.\n",
    "    Tr·∫£ v·ªÅ 1 n·∫øu c√≥, 0 n·∫øu kh√¥ng.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text = text.lower()\n",
    "    return int(any(re.search(rf\"(?<!\\w){re.escape(kw)}(?!\\w)\", text) for kw in keywords))\n",
    "\n",
    "# L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Chu·∫©n h√≥a m√¥ t·∫£:\n",
    "    - Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng\n",
    "    - B·ªè URL, k√Ω t·ª± ƒë·∫∑c bi·ªát, s·ªë\n",
    "    - Chu·∫©n h√≥a kho·∫£ng tr·∫Øng\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Lo·∫°i b·ªè stopwords ti·∫øng Vi·ªát\n",
    "vietnamese_stopwords = set([\n",
    "    \"xe\", \"m√°y\", \"b√°n\", \"c·∫ßn\", \"mua\", \"b√°o\", \"li√™n\", \"h·ªá\", \"anh\", \"ch·ªã\",\n",
    "    \"em\", \"mn\", \"m·ªçi\", \"ng∆∞·ªùi\", \"xin\", \"c·∫£m\", \"∆°n\", \"ch·ª£\", \"t·ªët\", \"ƒë·∫ßy\",\n",
    "    \"ƒë·ªß\", \"ƒëi·ªán\", \"tho·∫°i\", \"ƒë·ªãa\", \"ch·ªâ\", \"s·ªë\", \"c·ªßa\", \"v√†\", \"v·ªõi\", \"c√≤n\",\n",
    "    \"th√¨\", \"n√™n\", \"r·∫•t\", \"ƒë√£\", \"ƒë∆∞·ª£c\", \"ko\", \"kg\", \"th·∫≠t\", \"l√†\", \"th√¥i\",\n",
    "    \"nha\", \"nh√©\", \"·∫°\", \"nh∆∞ng\", \"b·ªüi\", \"v√¨\", \"th√¨\", \"n√†o\", \"v·∫≠y\"\n",
    "])\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    words = text.split()\n",
    "    return \" \".join([w for w in words if w not in vietnamese_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78c7768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Åp d·ª•ng NLP\n",
    "data_clean[\"desc_clean\"] = data_clean[\"M√¥ t·∫£ chi ti·∫øt\"].apply(clean_text)\n",
    "data_clean[\"desc_clean\"] = data_clean[\"desc_clean\"].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# √Åp d·ª•ng t·∫°o ƒë·∫∑c tr∆∞ng m·ªõi\n",
    "data_clean[\"is_moi\"] = data_clean[\"desc_clean\"].apply(lambda x: keyword_flag(x, kw_moi))\n",
    "data_clean[\"is_do_xe\"] = data_clean[\"desc_clean\"].apply(lambda x: keyword_flag(x, kw_do_xe))\n",
    "data_clean[\"is_su_dung_nhieu\"] = data_clean[\"desc_clean\"].apply(lambda x: keyword_flag(x, kw_su_dung))\n",
    "data_clean[\"is_bao_duong\"] = data_clean[\"desc_clean\"].apply(lambda x: keyword_flag(x, kw_bao_duong))\n",
    "data_clean[\"is_do_ben\"] = data_clean[\"desc_clean\"].apply(lambda x: keyword_flag(x, kw_do_ben))\n",
    "data_clean[\"is_phap_ly\"] = data_clean[\"desc_clean\"].apply(lambda x: keyword_flag(x, kw_phap_ly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b42a84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file = 'files/vietnamese-stopwords.txt'\n",
    "emojicon_file = 'files/emojicon.txt'\n",
    "teencode_file = 'files/teencode.txt'\n",
    "\n",
    "# Load stopwords, emojicons, teencode mappings\n",
    "with open(stop_word_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set([w.strip() for w in f.readlines() if w.strip()])\n",
    "\n",
    "with open(emojicon_file, 'r', encoding='utf-8') as f:\n",
    "    emojicons = [w.strip() for w in f.readlines() if w.strip()]\n",
    "\n",
    "with open(teencode_file, 'r', encoding='utf-8') as f:\n",
    "    teencode_map = {}\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 2:\n",
    "            teencode_map[parts[0]] = \" \".join(parts[1:])\n",
    "\n",
    "\n",
    "special_tokens = ['', ' ', ',', '.', '...', '-', ':', ';', '?', '%', '(', ')', '+', '/', \"'\", '&', '#', '*', '!', '\"', '_', '=', '[', ']', '{', '}', '~', '`', '|', '\\\\']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd50e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°c h√†m x·ª≠ l√Ω\n",
    "def remove_emojis(text):\n",
    "    for emo in emojicons:\n",
    "        text = text.replace(emo, ' ')\n",
    "    return text\n",
    "\n",
    "def normalize_teencode(text):\n",
    "    for key, val in teencode_map.items():\n",
    "        text = re.sub(rf'\\b{re.escape(key)}\\b', val, text)\n",
    "    return text\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # lo·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # lo·∫°i kho·∫£ng tr·∫Øng th·ª´a\n",
    "    return text\n",
    "\n",
    "# -----------------------\n",
    "# 4. T√ÅCH STOPWORD RI√äNG\n",
    "# -----------------------\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text, format=\"text\").split()\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# -----------------------\n",
    "# 5. CHU·∫®N H√ìA T·ªîNG H·ª¢P\n",
    "# -----------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = remove_emojis(text)\n",
    "    text = normalize_teencode(text)\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2e07b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Content'] = data_clean['M√¥ t·∫£ chi ti·∫øt'].apply(lambda x: ' '.join(x.split()[:200]))\n",
    "\n",
    "data_clean['clean_text'] = data_clean['Content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "752d8bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Ti√™u ƒë·ªÅ', 'Gi√°', 'Kho·∫£ng gi√° min', 'Kho·∫£ng gi√° max',\n",
       "       'M√¥ t·∫£ chi ti·∫øt', 'Th∆∞∆°ng hi·ªáu', 'D√≤ng xe', 'NƒÉm ƒëƒÉng k√Ω',\n",
       "       'S·ªë Km ƒë√£ ƒëi', 'Lo·∫°i xe', 'Dung t√≠ch xe', 'Xu·∫•t x·ª©', 'Ph√¢n kh√∫c gi√°',\n",
       "       'cc_numeric', 'price_segment_code', 'age', 'desc_clean', 'is_moi',\n",
       "       'is_do_xe', 'is_su_dung_nhieu', 'is_bao_duong', 'is_do_ben',\n",
       "       'is_phap_ly', 'Content', 'clean_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "014155f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    max_features=8000\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(data_clean['clean_text'])\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "def recommend(item_id: int, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    Recommend similar motorbikes based on cosine similarity.\n",
    "    Args:\n",
    "        item_id (int): id ho·∫∑c index c·ªßa xe trong DataFrame\n",
    "        top_n (int): s·ªë l∆∞·ª£ng g·ª£i √Ω mu·ªën l·∫•y\n",
    "    Returns:\n",
    "        DataFrame ch·ª©a c√°c xe t∆∞∆°ng t·ª±\n",
    "    \"\"\"\n",
    "    if item_id not in data.index:\n",
    "        raise ValueError(f\"id {item_id} kh√¥ng t·ªìn t·∫°i trong DataFrame\")\n",
    "\n",
    "    # L·∫•y h√†ng t∆∞∆°ng ·ª©ng trong ma tr·∫≠n cosine\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[item_id]))\n",
    "\n",
    "    # S·∫Øp x·∫øp theo ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·∫£m d·∫ßn, b·ªè ch√≠nh n√≥\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1: top_n + 1]\n",
    "\n",
    "    # L·∫•y index xe t∆∞∆°ng t·ª±\n",
    "    similar_indices = [i[0] for i in sim_scores]\n",
    "    similar_scores = [i[1] for i in sim_scores]\n",
    "\n",
    "    # T·∫°o DataFrame k·∫øt qu·∫£\n",
    "    recommendations = data.loc[similar_indices, ['id', 'Ti√™u ƒë·ªÅ', 'Content']].copy()\n",
    "    recommendations['similarity'] = similar_scores\n",
    "    return recommendations.reset_index(drop=True)\n",
    "\n",
    "def recommend_cosine_by_text(query: str, top_n: int = 5):\n",
    "    \"\"\"\n",
    "    G·ª£i √Ω xe m√°y t∆∞∆°ng t·ª± d·ª±a tr√™n vƒÉn b·∫£n ng∆∞·ªùi d√πng nh·∫≠p v√†o.\n",
    "    \n",
    "    Args:\n",
    "        query (str): vƒÉn b·∫£n t√¨m ki·∫øm\n",
    "        top_n (int): s·ªë l∆∞·ª£ng g·ª£i √Ω\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: danh s√°ch xe t∆∞∆°ng t·ª± + ƒë·ªô t∆∞∆°ng ƒë·ªìng\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Ti·ªÅn x·ª≠ l√Ω query b·∫±ng h√†m clean_text c·ªßa b·∫°n\n",
    "    clean_query = clean_text(query)\n",
    "\n",
    "    # 2. Vector h√≥a query\n",
    "    query_vec = vectorizer.transform([clean_query])\n",
    "\n",
    "    # 3. T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa query v√† to√†n b·ªô item\n",
    "    sims = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "\n",
    "    # 4. L·∫•y top N k·∫øt qu·∫£ cao nh·∫•t\n",
    "    top_idx = sims.argsort()[::-1][:top_n]\n",
    "    top_scores = sims[top_idx]\n",
    "\n",
    "    # 5. Tr·∫£ v·ªÅ DataFrame k·∫øt qu·∫£\n",
    "    result = data.iloc[top_idx][['id', 'Ti√™u ƒë·ªÅ', 'Content']].copy()\n",
    "    result[\"similarity\"] = top_scores\n",
    "\n",
    "    return result.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f9014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
